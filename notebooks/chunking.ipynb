{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "import gradio as gr\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "def load_and_chunk_text(input_path, chunk_size, chunk_overlap):\n",
    "    \"\"\"Load text and metadata from a file and perform chunking.\"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        extracted_data = json.load(f)\n",
    "\n",
    "    chunks = []\n",
    "    metadata = []\n",
    "\n",
    "    for entry in extracted_data:\n",
    "        text = entry[\"text\"]\n",
    "        source_metadata = {\n",
    "            \"source\": entry[\"source\"],\n",
    "            \"folder\": entry[\"folder\"],\n",
    "            \"file_name\": entry[\"file_name\"],\n",
    "            \"page\": entry[\"page\"],\n",
    "            \"title\": entry[\"title\"],\n",
    "            \"url\": entry[\"url\"]\n",
    "        }\n",
    "\n",
    "        # Chunk the text\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            metadata.append(source_metadata)\n",
    "            start += chunk_size - chunk_overlap\n",
    "\n",
    "    return chunks, metadata\n",
    "\n",
    "\n",
    "EXTRACTION_OUTPUT_PATH = r\"C:\\Users\\LEGION\\Desktop\\Project\\AI351\\PROJECT\\extracted_text.json\"\n",
    "\n",
    "# Chunking parameters\n",
    "CHUNK_SIZE = 10000\n",
    "CHUNK_OVERLAP = 2000\n",
    "documents, metadatas = load_and_chunk_text(EXTRACTION_OUTPUT_PATH,CHUNK_SIZE,CHUNK_OVERLAP  )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
