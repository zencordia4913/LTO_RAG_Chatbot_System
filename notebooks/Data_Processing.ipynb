{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "import pytesseract\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd \n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# Path to the dataset folder\n",
    "DATASET_PATH = r\"C:\\Users\\LEGION\\Desktop\\Project\\AI351\\PROJECT\\Dataset\"\n",
    "\n",
    "# DATASET_PATH = r\"C:\\Users\\LEGION\\Desktop\\Project\\AI351\\PROJECT\\Dataset\\Services/\"\n",
    "# Path to save extracted raw text and metadata\n",
    "EXTRACTION_OUTPUT_PATH = r\"C:\\Users\\LEGION\\Desktop\\Project\\AI351\\PROJECT\\extracted_text.json\"\n",
    "# Path to the CSV file containing additional metadata\n",
    "CSV_PATH = r\"C:\\Users\\LEGION\\Desktop\\Project\\AI351\\PROJECT\\data_dict.csv\"\n",
    "\n",
    "# Chunking parameters\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the extracted text by removing unwanted characters.\"\"\"\n",
    "    import re\n",
    "    # Replace multiple spaces and newlines with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # Remove leading/trailing spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_text_from_pdfs(folder_path, output_path):\n",
    "    \"\"\"Extract text and metadata from PDFs and save them to a file.\"\"\"\n",
    "    extracted_data = []\n",
    "    additional_metadata = pd.read_csv('data_dict.csv')\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                print(file)\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                folder_name = os.path.basename(root)\n",
    "                print(pdf_path)\n",
    "                # Get additional metadata from CSV if available\n",
    "                 \n",
    "                \n",
    "                doc = fitz.open(pdf_path)\n",
    "                for page_num, page in enumerate(doc, start=1):\n",
    "                    text = page.get_text()\n",
    "\n",
    "                    # If no text, fallback to OCR\n",
    "                    if not text.strip():\n",
    "                        pix = page.get_pixmap()\n",
    "                        img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
    "                        text = pytesseract.image_to_string(img)\n",
    "\n",
    "                    # Clean the text\n",
    "                    text = clean_text(text)\n",
    "                    if text:  # Ensure there's meaningful text after cleaning\n",
    "                        metadata = {\n",
    "                            \"source\": pdf_path,\n",
    "                            \"folder\": folder_name,\n",
    "                            \"file_name\": file,\n",
    "                            \"page\": page_num,\n",
    "                            \"title\": additional_metadata['Title'][additional_metadata['File Name']==file].iloc[0],\n",
    "                            \"url\": additional_metadata['Link'][additional_metadata['File Name']==file].iloc[0]\n",
    "                        }\n",
    "                        extracted_data.append({\n",
    "                            \"text\": text,\n",
    "                            **metadata\n",
    "                        })\n",
    "\n",
    "    # Save extracted data to a JSON file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(extracted_data, f, indent=4)\n",
    "\n",
    "    print(f\"Extracted data saved to {output_path}\")\n",
    "\n",
    "def load_and_chunk_text(input_path, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"Load text and metadata from a file and perform chunking.\"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        extracted_data = json.load(f)\n",
    "\n",
    "    chunks = []\n",
    "    metadata = []\n",
    "\n",
    "    for entry in extracted_data:\n",
    "        text = entry[\"text\"]\n",
    "        source_metadata = {\n",
    "            \"source\": entry[\"source\"],\n",
    "            \"folder\": entry[\"folder\"],\n",
    "            \"file_name\": entry[\"file_name\"],\n",
    "            \"page\": entry[\"page\"],\n",
    "            \"title\": entry[\"title\"],\n",
    "            \"url\": entry[\"url\"]\n",
    "        }\n",
    "\n",
    "        # Chunk the text\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            metadata.append(source_metadata)\n",
    "            start += chunk_size - chunk_overlap\n",
    "\n",
    "    return chunks, metadata\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Extract text and metadata from PDFs and save to a file\n",
    "print(\"Extracting text from PDFs...\")\n",
    "extract_text_from_pdfs(DATASET_PATH, EXTRACTION_OUTPUT_PATH)\n",
    "\n",
    "# Step 3: Load the extracted data and perform chunking\n",
    "print(\"Loading extracted data and performing chunking...\")\n",
    "chunks, metadatas = load_and_chunk_text(EXTRACTION_OUTPUT_PATH)\n",
    "\n",
    "# Output for debugging (optional)\n",
    "print(f\"Extracted {len(chunks)} text chunks.\")\n",
    "print(f\"Sample Metadata: {metadatas[:1]}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
